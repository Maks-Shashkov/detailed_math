{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMlULNDGpqIZKnJOOycvGiq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maks-Shashkov/detailed_math/blob/main/detailed_math1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ithraQOVFSSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Приводим все слова к нижнему регистру\n",
        "    text = text.lower()\n",
        "\n",
        "    # Удаляем знаки препинания\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Удаляем пробелы\n",
        "    text = text.replace(\" \", \"\")\n",
        "\n",
        "    return text\n",
        "\n",
        "# Пример использования\n",
        "input_text = \"Пример текста с ЗНАКАМИ препинания и пробелами!\"\n",
        "processed_text = preprocess_text(input_text)\n",
        "print(processed_text)"
      ],
      "metadata": {
        "id": "cAKjNFsyFXd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98fd1a3b-8dcb-4953-e02a-52f12b83177a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "примертекстасзнакамипрепинанияипробелами\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подсчитать частоту появления однобуквенных и двухбуквенных сочетаний."
      ],
      "metadata": {
        "id": "r8fu9XawJesQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_ngrams(text, n):\n",
        "    # Разбиваем текст на слова\n",
        "    words = text.split()\n",
        "\n",
        "    # Создаем словарь для хранения частоты появления сочетаний\n",
        "    ngrams_count = {}\n",
        "\n",
        "    # Проходим по каждому слову в тексте\n",
        "    for word in words:\n",
        "        # Пропускаем слова, которые короче, чем n\n",
        "        if len(word) < n:\n",
        "            continue\n",
        "\n",
        "        # Проходим по каждому сочетанию длины n в слове\n",
        "        for i in range(len(word) - n + 1):\n",
        "            ngram = word[i:i+n]\n",
        "            if ngram in ngrams_count:\n",
        "                ngrams_count[ngram] += 1\n",
        "            else:\n",
        "                ngrams_count[ngram] = 1\n",
        "\n",
        "    return ngrams_count\n",
        "\n",
        "# Пример использования\n",
        "input_text = \"Пример текста с однобуквенными и двухбуквенными сочетаниями.\"\n",
        "one_letter_count = count_ngrams(input_text, 1)\n",
        "two_letter_count = count_ngrams(input_text, 2)\n",
        "print(\"Частота появления однобуквенных сочетаний:\", one_letter_count)\n",
        "print(\"Частота появления двухбуквенных сочетаний:\", two_letter_count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTHHKXOLJiKa",
        "outputId": "8b9f3a3f-b02d-40a8-8c49-43677174071e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Частота появления однобуквенных сочетаний: {'П': 1, 'р': 2, 'и': 6, 'м': 4, 'е': 5, 'т': 3, 'к': 3, 'с': 3, 'а': 2, 'о': 3, 'д': 2, 'н': 6, 'б': 2, 'у': 3, 'в': 3, 'ы': 2, 'х': 1, 'ч': 1, 'я': 1, '.': 1}\n",
            "Частота появления двухбуквенных сочетаний: {'Пр': 1, 'ри': 1, 'им': 1, 'ме': 1, 'ер': 1, 'те': 1, 'ек': 1, 'кс': 1, 'ст': 1, 'та': 2, 'од': 1, 'дн': 1, 'но': 1, 'об': 1, 'бу': 2, 'ук': 2, 'кв': 2, 'ве': 2, 'ен': 2, 'нн': 2, 'ны': 2, 'ым': 2, 'ми': 3, 'дв': 1, 'ву': 1, 'ух': 1, 'хб': 1, 'со': 1, 'оч': 1, 'че': 1, 'ет': 1, 'ан': 1, 'ни': 1, 'ия': 1, 'ям': 1, 'и.': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Определить энтропию, приходящуюся в среднем на одну букву и на одно\n",
        "двухбуквенное сочетание."
      ],
      "metadata": {
        "id": "znFYSaC3KP00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def calculate_entropy(text):\n",
        "    # Разбиваем текст на символы\n",
        "    chars = list(text)\n",
        "\n",
        "    # Создаем словарь для хранения частоты появления каждого символа\n",
        "    char_count = {}\n",
        "    for char in chars:\n",
        "        if char in char_count:\n",
        "            char_count[char] += 1\n",
        "        else:\n",
        "            char_count[char] = 1\n",
        "\n",
        "    # Вычисляем энтропию для каждого символа\n",
        "    entropy_per_char = {}\n",
        "    for char in char_count:\n",
        "        probability = char_count[char] / len(chars)\n",
        "        entropy = -probability * math.log2(probability)\n",
        "        entropy_per_char[char] = entropy\n",
        "\n",
        "    # Вычисляем общую энтропию для всех символов\n",
        "    total_entropy = sum(entropy_per_char.values())\n",
        "\n",
        "    # Создаем словарь для хранения частоты появления каждого двухбуквенного сочетания\n",
        "    ngrams_count = {}\n",
        "    for i in range(len(chars) - 1):\n",
        "        ngram = chars[i:i+2]\n",
        "        if ngram in ngrams_count:\n",
        "            ngrams_count[ngram] += 1\n",
        "        else:\n",
        "            ngrams_count[ngram] = 1\n",
        "\n",
        "    # Вычисляем энтропию для каждого двухбуквенного сочетания\n",
        "    entropy_per_ngram = {}\n",
        "    for ngram in ngrams_count:\n",
        "        probability = ngrams_count[ngram] / (len(chars) - 1)\n",
        "        entropy = -probability * math.log2(probability)\n",
        "        entropy_per_ngram[ngram] = entropy\n",
        "\n",
        "    # Вычисляем общую энтропию для всех двухбуквенных сочетаний\n",
        "    total_ngram_entropy = sum(entropy_per_ngram.values())\n",
        "\n",
        "    return total_entropy / len(chars), total_ngram_entropy / (len(chars) - 1)\n",
        "\n",
        "input_text = \"Пример текста для расчета энтропии.\"\n",
        "char_entropy = calculate_entropy(input_text)\n",
        "print(\"Энтропия на одну букву:\", char_entropy / len(input_text))\n",
        "print(\"Энтропия на одно двухбуквенное сочетание:\", ngram_entropy / (len(input_text) - 1))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "F_wYGadoKQof",
        "outputId": "034bd525-5ba7-44f9-cb72-b8de9a4490fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-85c24b39f325>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Пример текста для расчета энтропии.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mchar_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Энтропия на одну букву:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_entropy\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Энтропия на одно двухбуквенное сочетание:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_entropy\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-85c24b39f325>\u001b[0m in \u001b[0;36mcalculate_entropy\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mngram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mngram\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mngrams_count\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mngrams_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Найти длину кода при равномерном побуквенном кодировании и избыточность."
      ],
      "metadata": {
        "id": "NBzx_EF7MgCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def calculate_uniform_coding_length(text):\n",
        "    # Разбиваем текст на символы\n",
        "    chars = list(text)\n",
        "\n",
        "    # Создаем множество уникальных символов\n",
        "    unique_chars = set(chars)\n",
        "\n",
        "    # Вычисляем длину кода при равномерном побуквенном кодировании\n",
        "    coding_length = math.ceil(math.log2(len(unique_chars)))\n",
        "\n",
        "    # Вычисляем избыточность\n",
        "    redundancy = coding_length / len(chars)\n",
        "\n",
        "    return coding_length, redundancy\n",
        "\n",
        "# Пример использования\n",
        "input_text = \"Пример текста для расчета длины кода.\"\n",
        "coding_length, redundancy = calculate_uniform_coding_length(input_text)\n",
        "print(\"Длина кода:\", coding_length)\n",
        "print(\"Избыточность:\", redundancy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG3rl2HZMgcd",
        "outputId": "dfd08c58-6887-4496-baaa-ff6354c5ae85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Длина кода: 5\n",
            "Избыточность: 0.13513513513513514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Удалить 20% наиболее часто встречающихся символов, проанализировать как\n",
        "изменится энтропия однобуквенных сочетаний и предположить, почему так\n",
        "происходит\n"
      ],
      "metadata": {
        "id": "fe5AgH7ANEyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "def remove_most_common_chars(text, percentage):\n",
        "    # Определяем количество символов, которые нужно удалить\n",
        "    char_count = Counter(text)\n",
        "    num_chars_to_remove = math.ceil(len(char_count) * percentage)\n",
        "\n",
        "    # Удаляем наиболее часто встречающиеся символы\n",
        "    most_common_chars = [char for char, count in char_count.most_common(num_chars_to_remove)]\n",
        "    cleaned_text = ''.join([char for char in text if char not in most_common_chars])\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "def calculate_entropy(text):\n",
        "    # Разбиваем текст на символы\n",
        "    chars = list(text)\n",
        "\n",
        "    # Создаем словарь для хранения частоты появления каждого символа\n",
        "    char_count = {}\n",
        "    for char in chars:\n",
        "        if char in char_count:\n",
        "            char_count[char] += 1\n",
        "        else:\n",
        "            char_count[char] = 1\n",
        "\n",
        "    # Вычисляем энтропию для каждого символа\n",
        "    entropy_per_char = {}\n",
        "    for char in char_count:\n",
        "        probability = char_count[char] / len(chars)\n",
        "        entropy = -probability * math.log2(probability)\n",
        "        entropy_per_char[char] = entropy\n",
        "\n",
        "    # Вычисляем общую энтропию для всех символов\n",
        "    total_entropy = sum(entropy_per_char.values())\n",
        "\n",
        "    return total_entropy\n",
        "\n",
        "# Пример использования\n",
        "input_text = \"Пример текста для удаления наиболее часто встречающихся символов.\"\n",
        "cleaned_text = remove_most_common_chars(input_text, 0.2)\n",
        "original_entropy = calculate_entropy(input_text)\n",
        "cleaned_entropy = calculate_entropy(cleaned_text)\n",
        "print(\"Энтропия исходного текста:\", original_entropy)\n",
        "print(\"Энтропия текста после удаления наиболее часто встречающихся символов:\", cleaned_entropy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFF33jzRNGWP",
        "outputId": "574a5859-ec95-44e3-9f38-a606cc59d77f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Энтропия исходного текста: 4.233831760897209\n",
            "Энтропия текста после удаления наиболее часто встречающихся символов: 3.95905708166975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Удалить 20% наиболее редко встречающихся символов, проанализировать как\n",
        "изменится энтропия однобуквенных сочетаний и предположить, почему так\n",
        "происходит."
      ],
      "metadata": {
        "id": "upZuLX4DNXxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "def remove_least_common_chars(text, percentage):\n",
        "    # Определяем количество символов, которые нужно удалить\n",
        "    char_count = Counter(text)\n",
        "    num_chars_to_remove = math.ceil(len(char_count) * percentage)\n",
        "\n",
        "    # Удаляем наименее часто встречающиеся символы\n",
        "    least_common_chars = [char for char, count in char_count.most_common()[:-num_chars_to_remove-1:-1]]\n",
        "    cleaned_text = ''.join([char for char in text if char not in least_common_chars])\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "def calculate_entropy(text):\n",
        "    # Разбиваем текст на символы\n",
        "    chars = list(text)\n",
        "\n",
        "    # Создаем словарь для хранения частоты появления каждого символа\n",
        "    char_count = {}\n",
        "    for char in chars:\n",
        "        if char in char_count:\n",
        "            char_count[char] += 1\n",
        "        else:\n",
        "            char_count[char] = 1\n",
        "\n",
        "    # Вычисляем энтропию для каждого символа\n",
        "    entropy_per_char = {}\n",
        "    for char in char_count:\n",
        "        probability = char_count[char] / len(chars)\n",
        "        entropy = -probability * math.log2(probability)\n",
        "        entropy_per_char[char] = entropy\n",
        "\n",
        "    # Вычисляем общую энтропию для всех символов\n",
        "    total_entropy = sum(entropy_per_char.values())\n",
        "\n",
        "    return total_entropy\n",
        "\n",
        "# Пример использования\n",
        "input_text = \"Пример текста для удаления наименее часто встречающихся символов.\"\n",
        "cleaned_text = remove_least_common_chars(input_text, 0.2)\n",
        "original_entropy = calculate_entropy(input_text)\n",
        "cleaned_entropy = calculate_entropy(cleaned_text)\n",
        "print([omitted])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "iJ8kxmY-NYfG",
        "outputId": "a49eb575-200e-4589-8062-3a93bfaf5ef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-8f14a3d867f3>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0moriginal_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mcleaned_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0momitted\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'omitted' is not defined"
          ]
        }
      ]
    }
  ]
}